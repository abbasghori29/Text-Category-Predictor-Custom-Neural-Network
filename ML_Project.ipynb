{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiq4StAQ98no"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import os\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.initialize_weights()\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        self.weights1 = np.random.randn(self.input_size, self.hidden_size) * np.sqrt(2.0/self.input_size)\n",
        "        self.weights2 = np.random.randn(self.hidden_size, self.output_size) * np.sqrt(2.0/self.hidden_size)\n",
        "        self.bias1 = np.zeros((1, self.hidden_size))\n",
        "        self.bias2 = np.zeros((1, self.output_size))\n",
        "\n",
        "    def get_parameters(self):\n",
        "        return {\n",
        "            'weights1': self.weights1.copy(),\n",
        "            'weights2': self.weights2.copy(),\n",
        "            'bias1': self.bias1.copy(),\n",
        "            'bias2': self.bias2.copy()\n",
        "        }\n",
        "\n",
        "    def set_parameters(self, parameters):\n",
        "        self.weights1 = parameters['weights1'].copy()\n",
        "        self.weights2 = parameters['weights2'].copy()\n",
        "        self.bias1 = parameters['bias1'].copy()\n",
        "        self.bias2 = parameters['bias2'].copy()\n",
        "\n",
        "    def relu(self, x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def relu_derivative(self, x):\n",
        "        return np.where(x > 0, 1, 0)\n",
        "\n",
        "    def softmax(self, x):\n",
        "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.layer1 = np.dot(X, self.weights1) + self.bias1\n",
        "        self.layer1_activation = self.relu(self.layer1)\n",
        "        self.layer2 = np.dot(self.layer1_activation, self.weights2) + self.bias2\n",
        "        self.output = self.softmax(self.layer2)\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, X, y, output):\n",
        "        batch_size = X.shape[0]\n",
        "        self.output_error = output - y\n",
        "\n",
        "        self.layer2_delta = self.output_error\n",
        "        self.layer1_error = np.dot(self.layer2_delta, self.weights2.T)\n",
        "        self.layer1_delta = self.layer1_error * self.relu_derivative(self.layer1)\n",
        "\n",
        "        # Add L2 regularization\n",
        "        lambda_reg = 0.01\n",
        "        self.weights2 -= self.learning_rate * (np.dot(self.layer1_activation.T, self.layer2_delta) / batch_size + lambda_reg * self.weights2)\n",
        "        self.bias2 -= self.learning_rate * np.mean(self.layer2_delta, axis=0, keepdims=True)\n",
        "        self.weights1 -= self.learning_rate * (np.dot(X.T, self.layer1_delta) / batch_size + lambda_reg * self.weights1)\n",
        "        self.bias1 -= self.learning_rate * np.mean(self.layer1_delta, axis=0, keepdims=True)\n",
        "\n",
        "    def calculate_loss(self, y_true, y_pred):\n",
        "        return -np.mean(np.sum(y_true * np.log(y_pred + 1e-15), axis=1))\n",
        "\n",
        "    def train_batch(self, X, y, batch_size=32):\n",
        "        indices = np.random.permutation(len(X))\n",
        "        total_loss = 0\n",
        "\n",
        "        for start_idx in range(0, len(X), batch_size):\n",
        "            batch_indices = indices[start_idx:start_idx + batch_size]\n",
        "            X_batch = X[batch_indices]\n",
        "            y_batch = y[batch_indices]\n",
        "\n",
        "            output = self.forward(X_batch)\n",
        "            self.backward(X_batch, y_batch, output)\n",
        "            total_loss += self.calculate_loss(y_batch, output)\n",
        "\n",
        "        return total_loss / (len(X) // batch_size)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase and remove special characters\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', str(text).lower())\n",
        "    return text\n",
        "\n",
        "class ModelCheckpoint:\n",
        "    def __init__(self, filepath, verbose=1):\n",
        "        self.filepath = filepath\n",
        "        self.verbose = verbose\n",
        "        self.best_val_loss = float('inf')\n",
        "\n",
        "    def save_checkpoint(self, model, val_loss, epoch):\n",
        "        if val_loss < self.best_val_loss:\n",
        "            self.best_val_loss = val_loss\n",
        "            parameters = model.get_parameters()\n",
        "            with open(self.filepath, 'wb') as f:\n",
        "                pickle.dump(parameters, f)\n",
        "            if self.verbose:\n",
        "                print(f'\\nEpoch {epoch}: Validation loss improved to {val_loss:.4f}, saving model to {self.filepath}')\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "def load_and_preprocess_data(train_path, test_path):\n",
        "    # Load datasets\n",
        "    train_df = pd.read_csv(train_path)\n",
        "    test_df = pd.read_csv(test_path)\n",
        "\n",
        "    # Combine title and description\n",
        "    train_df['text'] = train_df['Title'] + ' ' + train_df['Description']\n",
        "    test_df['text'] = test_df['Title'] + ' ' + test_df['Description']\n",
        "\n",
        "    # Preprocess text\n",
        "    train_df['text'] = train_df['text'].apply(preprocess_text)\n",
        "    test_df['text'] = test_df['text'].apply(preprocess_text)\n",
        "\n",
        "    # Convert class index to one-hot encoding\n",
        "    label_encoder = LabelEncoder()\n",
        "    train_labels = label_encoder.fit_transform(train_df['Class Index'] - 1)\n",
        "    test_labels = label_encoder.transform(test_df['Class Index'] - 1)\n",
        "\n",
        "    train_labels_onehot = np.eye(4)[train_labels]\n",
        "    test_labels_onehot = np.eye(4)[test_labels]\n",
        "\n",
        "    # TF-IDF Vectorization\n",
        "    vectorizer = TfidfVectorizer(max_features=5000)\n",
        "    X_train = vectorizer.fit_transform(train_df['text']).toarray()\n",
        "    X_test = vectorizer.transform(test_df['text']).toarray()\n",
        "\n",
        "    # Save vectorizer to Google Drive\n",
        "    vectorizer_path = '/content/drive/MyDrive/ML Proj Dataset/tfidf_vectorizer.pkl'\n",
        "    with open(vectorizer_path, 'wb') as f:\n",
        "        pickle.dump(vectorizer, f)\n",
        "    print(f\"Vectorizer saved to: {vectorizer_path}\")\n",
        "\n",
        "    return X_train, train_labels_onehot, X_test, test_labels_onehot, label_encoder\n",
        "\n",
        "def create_training_curves(train_losses, val_losses, output_dir):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.title('Training and Validation Loss over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig(os.path.join(output_dir, 'training_curves.png'))\n",
        "    plt.close()\n",
        "\n",
        "def main():\n",
        "    # Create directory for model artifacts in Google Drive\n",
        "    base_dir = '/content/drive/MyDrive/ML Proj Dataset'\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    output_dir = os.path.join(base_dir, f'model_artifacts_{timestamp}')\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Load and preprocess data\n",
        "    X_train, y_train, X_test, y_test, label_encoder = load_and_preprocess_data(\n",
        "        os.path.join(base_dir, 'train.csv'),\n",
        "        os.path.join(base_dir, 'test.csv')\n",
        "    )\n",
        "\n",
        "    # Split training data into train and validation sets\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Neural Network parameters\n",
        "    input_size = X_train.shape[1]\n",
        "    hidden_size = 256\n",
        "    output_size = 4\n",
        "    learning_rate = 0.001\n",
        "    epochs = 100\n",
        "    batch_size = 32\n",
        "    early_stopping_patience = 10\n",
        "\n",
        "    # Initialize neural network\n",
        "    nn = NeuralNetwork(input_size, hidden_size, output_size, learning_rate)\n",
        "\n",
        "    # Initialize checkpoint with updated path\n",
        "    checkpoint = ModelCheckpoint(os.path.join(output_dir, 'best_model.pkl'))\n",
        "\n",
        "    # Training loop with early stopping\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        train_loss = nn.train_batch(X_train, y_train, batch_size)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        # Validation\n",
        "        val_predictions = nn.forward(X_val)\n",
        "        val_loss = nn.calculate_loss(y_val, val_predictions)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        # Save checkpoint if validation loss improves\n",
        "        if checkpoint.save_checkpoint(nn, val_loss, epoch):\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        # Print progress\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "        # Early stopping\n",
        "        if patience_counter >= early_stopping_patience:\n",
        "            print(f'\\nEarly stopping triggered after {epoch+1} epochs')\n",
        "            break\n",
        "\n",
        "    # Create and save training curves\n",
        "    create_training_curves(train_losses, val_losses, output_dir)\n",
        "\n",
        "    # Load best model for evaluation\n",
        "    with open(os.path.join(output_dir, 'best_model.pkl'), 'rb') as f:\n",
        "        best_parameters = pickle.load(f)\n",
        "    nn.set_parameters(best_parameters)\n",
        "\n",
        "    # Evaluate on test set\n",
        "    test_predictions = nn.forward(X_test)\n",
        "    predicted_labels = np.argmax(test_predictions, axis=1)\n",
        "    true_labels = np.argmax(y_test, axis=1)\n",
        "\n",
        "    # Calculate and save metrics\n",
        "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "    class_names = ['World', 'Sports', 'Business', 'Science']\n",
        "    classification_rep = classification_report(true_labels, predicted_labels, target_names=class_names)\n",
        "\n",
        "    # Save results to file\n",
        "    results_file = os.path.join(output_dir, 'results.txt')\n",
        "    with open(results_file, 'w') as f:\n",
        "        f.write(f'Test Accuracy: {accuracy * 100:.2f}%\\n\\n')\n",
        "        f.write('Classification Report:\\n')\n",
        "        f.write(classification_rep)\n",
        "\n",
        "    print(f'\\nTest Accuracy: {accuracy * 100:.2f}%')\n",
        "    print('\\nClassification Report:')\n",
        "    print(classification_rep)\n",
        "    print(f'\\nAll model artifacts saved in: {output_dir}')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZGJ1UVc-3RZ",
        "outputId": "71db6ac9-4c1e-49ef-c45f-0247f4fb3674"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Vectorizer saved to: /content/drive/MyDrive/ML Proj Dataset/tfidf_vectorizer.pkl\n",
            "\n",
            "Epoch 0: Validation loss improved to 1.3824, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 1/100, Train Loss: 1.3845, Val Loss: 1.3824\n",
            "\n",
            "Epoch 1: Validation loss improved to 1.3784, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 2/100, Train Loss: 1.3804, Val Loss: 1.3784\n",
            "\n",
            "Epoch 2: Validation loss improved to 1.3742, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 3/100, Train Loss: 1.3762, Val Loss: 1.3742\n",
            "\n",
            "Epoch 3: Validation loss improved to 1.3698, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 4/100, Train Loss: 1.3719, Val Loss: 1.3698\n",
            "\n",
            "Epoch 4: Validation loss improved to 1.3653, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 5/100, Train Loss: 1.3673, Val Loss: 1.3653\n",
            "\n",
            "Epoch 5: Validation loss improved to 1.3607, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 6/100, Train Loss: 1.3626, Val Loss: 1.3607\n",
            "\n",
            "Epoch 6: Validation loss improved to 1.3560, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 7/100, Train Loss: 1.3579, Val Loss: 1.3560\n",
            "\n",
            "Epoch 7: Validation loss improved to 1.3513, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 8/100, Train Loss: 1.3531, Val Loss: 1.3513\n",
            "\n",
            "Epoch 8: Validation loss improved to 1.3465, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 9/100, Train Loss: 1.3483, Val Loss: 1.3465\n",
            "\n",
            "Epoch 9: Validation loss improved to 1.3416, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 10/100, Train Loss: 1.3434, Val Loss: 1.3416\n",
            "\n",
            "Epoch 10: Validation loss improved to 1.3365, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 11/100, Train Loss: 1.3383, Val Loss: 1.3365\n",
            "\n",
            "Epoch 11: Validation loss improved to 1.3313, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 12/100, Train Loss: 1.3331, Val Loss: 1.3313\n",
            "\n",
            "Epoch 12: Validation loss improved to 1.3259, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 13/100, Train Loss: 1.3277, Val Loss: 1.3259\n",
            "\n",
            "Epoch 13: Validation loss improved to 1.3203, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 14/100, Train Loss: 1.3221, Val Loss: 1.3203\n",
            "\n",
            "Epoch 14: Validation loss improved to 1.3145, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 15/100, Train Loss: 1.3163, Val Loss: 1.3145\n",
            "\n",
            "Epoch 15: Validation loss improved to 1.3084, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 16/100, Train Loss: 1.3102, Val Loss: 1.3084\n",
            "\n",
            "Epoch 16: Validation loss improved to 1.3020, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 17/100, Train Loss: 1.3038, Val Loss: 1.3020\n",
            "\n",
            "Epoch 17: Validation loss improved to 1.2953, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 18/100, Train Loss: 1.2972, Val Loss: 1.2953\n",
            "\n",
            "Epoch 18: Validation loss improved to 1.2882, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 19/100, Train Loss: 1.2902, Val Loss: 1.2882\n",
            "\n",
            "Epoch 19: Validation loss improved to 1.2807, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 20/100, Train Loss: 1.2828, Val Loss: 1.2807\n",
            "\n",
            "Epoch 20: Validation loss improved to 1.2731, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 21/100, Train Loss: 1.2751, Val Loss: 1.2731\n",
            "\n",
            "Epoch 21: Validation loss improved to 1.2648, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 22/100, Train Loss: 1.2670, Val Loss: 1.2648\n",
            "\n",
            "Epoch 22: Validation loss improved to 1.2562, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 23/100, Train Loss: 1.2585, Val Loss: 1.2562\n",
            "\n",
            "Epoch 23: Validation loss improved to 1.2473, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 24/100, Train Loss: 1.2496, Val Loss: 1.2473\n",
            "\n",
            "Epoch 24: Validation loss improved to 1.2379, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 25/100, Train Loss: 1.2403, Val Loss: 1.2379\n",
            "\n",
            "Epoch 25: Validation loss improved to 1.2282, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 26/100, Train Loss: 1.2306, Val Loss: 1.2282\n",
            "\n",
            "Epoch 26: Validation loss improved to 1.2181, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 27/100, Train Loss: 1.2206, Val Loss: 1.2181\n",
            "\n",
            "Epoch 27: Validation loss improved to 1.2076, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 28/100, Train Loss: 1.2102, Val Loss: 1.2076\n",
            "\n",
            "Epoch 28: Validation loss improved to 1.1969, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 29/100, Train Loss: 1.1994, Val Loss: 1.1969\n",
            "\n",
            "Epoch 29: Validation loss improved to 1.1858, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 30/100, Train Loss: 1.1883, Val Loss: 1.1858\n",
            "\n",
            "Epoch 30: Validation loss improved to 1.1745, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 31/100, Train Loss: 1.1769, Val Loss: 1.1745\n",
            "\n",
            "Epoch 31: Validation loss improved to 1.1628, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 32/100, Train Loss: 1.1652, Val Loss: 1.1628\n",
            "\n",
            "Epoch 32: Validation loss improved to 1.1510, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 33/100, Train Loss: 1.1534, Val Loss: 1.1510\n",
            "\n",
            "Epoch 33: Validation loss improved to 1.1390, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 34/100, Train Loss: 1.1413, Val Loss: 1.1390\n",
            "\n",
            "Epoch 34: Validation loss improved to 1.1269, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 35/100, Train Loss: 1.1291, Val Loss: 1.1269\n",
            "\n",
            "Epoch 35: Validation loss improved to 1.1147, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 36/100, Train Loss: 1.1167, Val Loss: 1.1147\n",
            "\n",
            "Epoch 36: Validation loss improved to 1.1025, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 37/100, Train Loss: 1.1044, Val Loss: 1.1025\n",
            "\n",
            "Epoch 37: Validation loss improved to 1.0903, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 38/100, Train Loss: 1.0920, Val Loss: 1.0903\n",
            "\n",
            "Epoch 38: Validation loss improved to 1.0781, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 39/100, Train Loss: 1.0796, Val Loss: 1.0781\n",
            "\n",
            "Epoch 39: Validation loss improved to 1.0660, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 40/100, Train Loss: 1.0673, Val Loss: 1.0660\n",
            "\n",
            "Epoch 40: Validation loss improved to 1.0541, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 41/100, Train Loss: 1.0551, Val Loss: 1.0541\n",
            "\n",
            "Epoch 41: Validation loss improved to 1.0422, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 42/100, Train Loss: 1.0431, Val Loss: 1.0422\n",
            "\n",
            "Epoch 42: Validation loss improved to 1.0305, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 43/100, Train Loss: 1.0312, Val Loss: 1.0305\n",
            "\n",
            "Epoch 43: Validation loss improved to 1.0192, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 44/100, Train Loss: 1.0195, Val Loss: 1.0192\n",
            "\n",
            "Epoch 44: Validation loss improved to 1.0080, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 45/100, Train Loss: 1.0081, Val Loss: 1.0080\n",
            "\n",
            "Epoch 45: Validation loss improved to 0.9971, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 46/100, Train Loss: 0.9969, Val Loss: 0.9971\n",
            "\n",
            "Epoch 46: Validation loss improved to 0.9864, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 47/100, Train Loss: 0.9859, Val Loss: 0.9864\n",
            "\n",
            "Epoch 47: Validation loss improved to 0.9761, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 48/100, Train Loss: 0.9753, Val Loss: 0.9761\n",
            "\n",
            "Epoch 48: Validation loss improved to 0.9660, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 49/100, Train Loss: 0.9650, Val Loss: 0.9660\n",
            "\n",
            "Epoch 49: Validation loss improved to 0.9563, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 50/100, Train Loss: 0.9549, Val Loss: 0.9563\n",
            "\n",
            "Epoch 50: Validation loss improved to 0.9469, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 51/100, Train Loss: 0.9452, Val Loss: 0.9469\n",
            "\n",
            "Epoch 51: Validation loss improved to 0.9377, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 52/100, Train Loss: 0.9359, Val Loss: 0.9377\n",
            "\n",
            "Epoch 52: Validation loss improved to 0.9290, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 53/100, Train Loss: 0.9268, Val Loss: 0.9290\n",
            "\n",
            "Epoch 53: Validation loss improved to 0.9206, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 54/100, Train Loss: 0.9181, Val Loss: 0.9206\n",
            "\n",
            "Epoch 54: Validation loss improved to 0.9124, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 55/100, Train Loss: 0.9096, Val Loss: 0.9124\n",
            "\n",
            "Epoch 55: Validation loss improved to 0.9045, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 56/100, Train Loss: 0.9015, Val Loss: 0.9045\n",
            "\n",
            "Epoch 56: Validation loss improved to 0.8970, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 57/100, Train Loss: 0.8938, Val Loss: 0.8970\n",
            "\n",
            "Epoch 57: Validation loss improved to 0.8899, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 58/100, Train Loss: 0.8863, Val Loss: 0.8899\n",
            "\n",
            "Epoch 58: Validation loss improved to 0.8828, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 59/100, Train Loss: 0.8791, Val Loss: 0.8828\n",
            "\n",
            "Epoch 59: Validation loss improved to 0.8762, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 60/100, Train Loss: 0.8722, Val Loss: 0.8762\n",
            "\n",
            "Epoch 60: Validation loss improved to 0.8698, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 61/100, Train Loss: 0.8656, Val Loss: 0.8698\n",
            "\n",
            "Epoch 61: Validation loss improved to 0.8637, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 62/100, Train Loss: 0.8593, Val Loss: 0.8637\n",
            "\n",
            "Epoch 62: Validation loss improved to 0.8578, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 63/100, Train Loss: 0.8533, Val Loss: 0.8578\n",
            "\n",
            "Epoch 63: Validation loss improved to 0.8523, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 64/100, Train Loss: 0.8475, Val Loss: 0.8523\n",
            "\n",
            "Epoch 64: Validation loss improved to 0.8470, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 65/100, Train Loss: 0.8419, Val Loss: 0.8470\n",
            "\n",
            "Epoch 65: Validation loss improved to 0.8419, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 66/100, Train Loss: 0.8367, Val Loss: 0.8419\n",
            "\n",
            "Epoch 66: Validation loss improved to 0.8370, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 67/100, Train Loss: 0.8316, Val Loss: 0.8370\n",
            "\n",
            "Epoch 67: Validation loss improved to 0.8323, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 68/100, Train Loss: 0.8267, Val Loss: 0.8323\n",
            "\n",
            "Epoch 68: Validation loss improved to 0.8280, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 69/100, Train Loss: 0.8221, Val Loss: 0.8280\n",
            "\n",
            "Epoch 69: Validation loss improved to 0.8235, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 70/100, Train Loss: 0.8177, Val Loss: 0.8235\n",
            "\n",
            "Epoch 70: Validation loss improved to 0.8195, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 71/100, Train Loss: 0.8135, Val Loss: 0.8195\n",
            "\n",
            "Epoch 71: Validation loss improved to 0.8156, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 72/100, Train Loss: 0.8095, Val Loss: 0.8156\n",
            "\n",
            "Epoch 72: Validation loss improved to 0.8119, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 73/100, Train Loss: 0.8056, Val Loss: 0.8119\n",
            "\n",
            "Epoch 73: Validation loss improved to 0.8083, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 74/100, Train Loss: 0.8019, Val Loss: 0.8083\n",
            "\n",
            "Epoch 74: Validation loss improved to 0.8050, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 75/100, Train Loss: 0.7984, Val Loss: 0.8050\n",
            "\n",
            "Epoch 75: Validation loss improved to 0.8017, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 76/100, Train Loss: 0.7950, Val Loss: 0.8017\n",
            "\n",
            "Epoch 76: Validation loss improved to 0.7986, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 77/100, Train Loss: 0.7918, Val Loss: 0.7986\n",
            "\n",
            "Epoch 77: Validation loss improved to 0.7957, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 78/100, Train Loss: 0.7887, Val Loss: 0.7957\n",
            "\n",
            "Epoch 78: Validation loss improved to 0.7928, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 79/100, Train Loss: 0.7858, Val Loss: 0.7928\n",
            "\n",
            "Epoch 79: Validation loss improved to 0.7901, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 80/100, Train Loss: 0.7830, Val Loss: 0.7901\n",
            "\n",
            "Epoch 80: Validation loss improved to 0.7876, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 81/100, Train Loss: 0.7803, Val Loss: 0.7876\n",
            "\n",
            "Epoch 81: Validation loss improved to 0.7850, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 82/100, Train Loss: 0.7777, Val Loss: 0.7850\n",
            "\n",
            "Epoch 82: Validation loss improved to 0.7829, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 83/100, Train Loss: 0.7752, Val Loss: 0.7829\n",
            "\n",
            "Epoch 83: Validation loss improved to 0.7806, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 84/100, Train Loss: 0.7730, Val Loss: 0.7806\n",
            "\n",
            "Epoch 84: Validation loss improved to 0.7783, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 85/100, Train Loss: 0.7707, Val Loss: 0.7783\n",
            "\n",
            "Epoch 85: Validation loss improved to 0.7762, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 86/100, Train Loss: 0.7685, Val Loss: 0.7762\n",
            "\n",
            "Epoch 86: Validation loss improved to 0.7744, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 87/100, Train Loss: 0.7665, Val Loss: 0.7744\n",
            "\n",
            "Epoch 87: Validation loss improved to 0.7724, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 88/100, Train Loss: 0.7646, Val Loss: 0.7724\n",
            "\n",
            "Epoch 88: Validation loss improved to 0.7706, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 89/100, Train Loss: 0.7627, Val Loss: 0.7706\n",
            "\n",
            "Epoch 89: Validation loss improved to 0.7689, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 90/100, Train Loss: 0.7609, Val Loss: 0.7689\n",
            "\n",
            "Epoch 90: Validation loss improved to 0.7672, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 91/100, Train Loss: 0.7592, Val Loss: 0.7672\n",
            "\n",
            "Epoch 91: Validation loss improved to 0.7657, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 92/100, Train Loss: 0.7575, Val Loss: 0.7657\n",
            "\n",
            "Epoch 92: Validation loss improved to 0.7643, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 93/100, Train Loss: 0.7559, Val Loss: 0.7643\n",
            "\n",
            "Epoch 93: Validation loss improved to 0.7627, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 94/100, Train Loss: 0.7545, Val Loss: 0.7627\n",
            "\n",
            "Epoch 94: Validation loss improved to 0.7615, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 95/100, Train Loss: 0.7530, Val Loss: 0.7615\n",
            "\n",
            "Epoch 95: Validation loss improved to 0.7601, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 96/100, Train Loss: 0.7517, Val Loss: 0.7601\n",
            "\n",
            "Epoch 96: Validation loss improved to 0.7587, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 97/100, Train Loss: 0.7504, Val Loss: 0.7587\n",
            "\n",
            "Epoch 97: Validation loss improved to 0.7575, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 98/100, Train Loss: 0.7491, Val Loss: 0.7575\n",
            "\n",
            "Epoch 98: Validation loss improved to 0.7564, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 99/100, Train Loss: 0.7478, Val Loss: 0.7564\n",
            "\n",
            "Epoch 99: Validation loss improved to 0.7552, saving model to /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\n",
            "Epoch 100/100, Train Loss: 0.7467, Val Loss: 0.7552\n",
            "\n",
            "Test Accuracy: 85.53%\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       World       0.88      0.87      0.87      1900\n",
            "      Sports       0.89      0.96      0.92      1900\n",
            "    Business       0.84      0.78      0.81      1900\n",
            "     Science       0.81      0.81      0.81      1900\n",
            "\n",
            "    accuracy                           0.86      7600\n",
            "   macro avg       0.85      0.86      0.85      7600\n",
            "weighted avg       0.85      0.86      0.85      7600\n",
            "\n",
            "\n",
            "All model artifacts saved in: /content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "class TextPredictor:\n",
        "    def __init__(self, model_path, vectorizer_path):\n",
        "        \"\"\"\n",
        "        Initialize the predictor with paths to the saved model and vectorizer.\n",
        "\n",
        "        Args:\n",
        "            model_path (str): Path to the saved model pickle file\n",
        "            vectorizer_path (str): Path to the saved vectorizer pickle file\n",
        "        \"\"\"\n",
        "        self.class_names = ['World', 'Sports', 'Business', 'Science']\n",
        "        self.model = None\n",
        "        self.vectorizer = None\n",
        "        self.load_artifacts(model_path, vectorizer_path)\n",
        "\n",
        "    def load_artifacts(self, model_path, vectorizer_path):\n",
        "        \"\"\"Load the saved model and vectorizer.\"\"\"\n",
        "        try:\n",
        "            # Load the vectorizer\n",
        "            with open(vectorizer_path, 'rb') as f:\n",
        "                self.vectorizer = pickle.load(f)\n",
        "\n",
        "            # Load the model parameters\n",
        "            with open(model_path, 'rb') as f:\n",
        "                model_params = pickle.load(f)\n",
        "\n",
        "            # Recreate the neural network with loaded parameters\n",
        "            input_size = model_params['weights1'].shape[0]\n",
        "            hidden_size = model_params['weights1'].shape[1]\n",
        "            output_size = model_params['weights2'].shape[1]\n",
        "\n",
        "            self.model = NeuralNetwork(input_size, hidden_size, output_size)\n",
        "            self.model.set_parameters(model_params)\n",
        "\n",
        "            print(\"Model and vectorizer loaded successfully!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"Error loading model artifacts: {str(e)}\")\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"Preprocess the input text.\"\"\"\n",
        "        # Convert to lowercase and remove special characters\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', '', str(text).lower())\n",
        "        return text\n",
        "\n",
        "    def predict(self, text):\n",
        "        \"\"\"\n",
        "        Predict the category of the input text.\n",
        "\n",
        "        Args:\n",
        "            text (str): Input text to classify\n",
        "\n",
        "        Returns:\n",
        "            dict: Prediction results containing category and confidence scores\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Preprocess the text\n",
        "            processed_text = self.preprocess_text(text)\n",
        "\n",
        "            # Transform text using the vectorizer\n",
        "            text_vectorized = self.vectorizer.transform([processed_text]).toarray()\n",
        "\n",
        "            # Get model predictions\n",
        "            predictions = self.model.forward(text_vectorized)\n",
        "\n",
        "            # Get the predicted class and probabilities\n",
        "            predicted_class_idx = np.argmax(predictions[0])\n",
        "            probabilities = predictions[0]\n",
        "\n",
        "            # Create results dictionary\n",
        "            results = {\n",
        "                'predicted_category': self.class_names[predicted_class_idx],\n",
        "                'confidence': float(probabilities[predicted_class_idx]),\n",
        "                'probabilities': {\n",
        "                    category: float(prob)\n",
        "                    for category, prob in zip(self.class_names, probabilities)\n",
        "                }\n",
        "            }\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"Error making prediction: {str(e)}\")\n",
        "\n",
        "class NeuralNetwork:\n",
        "    \"\"\"Simplified version of the neural network for prediction only.\"\"\"\n",
        "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.initialize_weights()\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        self.weights1 = np.zeros((self.input_size, self.hidden_size))\n",
        "        self.weights2 = np.zeros((self.hidden_size, self.output_size))\n",
        "        self.bias1 = np.zeros((1, self.hidden_size))\n",
        "        self.bias2 = np.zeros((1, self.output_size))\n",
        "\n",
        "    def set_parameters(self, parameters):\n",
        "        self.weights1 = parameters['weights1'].copy()\n",
        "        self.weights2 = parameters['weights2'].copy()\n",
        "        self.bias1 = parameters['bias1'].copy()\n",
        "        self.bias2 = parameters['bias2'].copy()\n",
        "\n",
        "    def relu(self, x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def softmax(self, x):\n",
        "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "    def forward(self, X):\n",
        "        layer1 = np.dot(X, self.weights1) + self.bias1\n",
        "        layer1_activation = self.relu(layer1)\n",
        "        layer2 = np.dot(layer1_activation, self.weights2) + self.bias2\n",
        "        output = self.softmax(layer2)\n",
        "        return output"
      ],
      "metadata": {
        "id": "UxLTINFDUxsU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. First, set up your file paths correctly\n",
        "model_path = \"/content/drive/MyDrive/ML Proj Dataset/model_artifacts_20241125_082544/best_model.pkl\"    # Replace with your model file path\n",
        "vectorizer_path = \"/content/drive/MyDrive/ML Proj Dataset/tfidf_vectorizer.pkl\"  # Replace with your vectorizer file path\n",
        "\n",
        "# 2. Create the predictor\n",
        "predictor = TextPredictor(model_path, vectorizer_path)\n",
        "\n",
        "# 3. Input your sentence and get prediction\n",
        "sentence = \"\"\"\n",
        "While the synergy between our vertically integrated strategies continues to amplify market penetration,\n",
        "the Q4 projections remain contingent on the elasticity of discretionary consumer spending amidst fluctuating interest rate policies.\n",
        "\"\"\"  # Put your sentence here\n",
        "result = predictor.predict(sentence)\n",
        "\n",
        "# 4. See the results\n",
        "print(f\"\\nText: {sentence}\")\n",
        "print(f\"Predicted Category: {result['predicted_category']}\")\n",
        "print(f\"Confidence: {result['confidence']:.2%}\")\n",
        "print(\"\\nProbabilities for all categories:\")\n",
        "for category, prob in result['probabilities'].items():\n",
        "    print(f\"{category}: {prob:.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4anSJOw3tah",
        "outputId": "f29dda20-893c-43a6-95e7-350d9be4d6a4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and vectorizer loaded successfully!\n",
            "\n",
            "Text: \n",
            "While the synergy between our vertically integrated strategies continues to amplify market penetration, \n",
            "the Q4 projections remain contingent on the elasticity of discretionary consumer spending amidst fluctuating interest rate policies.\n",
            "\n",
            "Predicted Category: Business\n",
            "Confidence: 38.42%\n",
            "\n",
            "Probabilities for all categories:\n",
            "World: 16.87%\n",
            "Sports: 14.65%\n",
            "Business: 38.42%\n",
            "Science: 30.06%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Check the folder in Google Drive\n",
        "output_dir = '/content/drive/MyDrive/model_artifacts_20231125_153015'\n",
        "if os.path.exists(output_dir):\n",
        "    print(f'Folder found: {output_dir}')\n",
        "else:\n",
        "    print(f'Folder not found: {output_dir}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7UMrGsaZblF",
        "outputId": "b33ef849-b9c9-40ba-b452-dc3de6a28ea5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder not found: /content/drive/MyDrive/model_artifacts_20231125_153015\n"
          ]
        }
      ]
    }
  ]
}